{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from torch.distributions import Categorical\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import gc\n",
    "\n",
    "from mcts1 import MCTS1\n",
    "from tangram import Tangram\n",
    "from mctstree import MCTSTree\n",
    "from itertools import count\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serialization_path:  ./models/mcts1/hierarchical_blocks_2_n_samples_20_unbiased_dataset_network_seed_543\n"
     ]
    }
   ],
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "c = np.sqrt(2)\n",
    "gamma=0.9\n",
    "environment_seed=543\n",
    "network_seed=543\n",
    "render=False\n",
    "log_interval=1\n",
    "gpu=False\n",
    "load_agent=True\n",
    "\n",
    "n_grid = 20\n",
    "n_blocks = 4\n",
    "n_possible_blocks = 6\n",
    "chunk_type = 7\n",
    "n_blocks_H = 2\n",
    "n_distinct_samples = 20\n",
    "n_samples = 20\n",
    "\n",
    "action_dims = [3,n_blocks,n_possible_blocks]\n",
    "\n",
    "n_simuls = 10\n",
    "n_evals = 100\n",
    "\n",
    "serialization_path = './models/mcts1/hierarchical_blocks_{}_n_samples_{}_unbiased_dataset_network_seed_{}'.format(n_blocks_H, n_samples, network_seed)\n",
    "print('serialization_path: ',serialization_path)\n",
    "serialize_every_n_episodes = 10000\n",
    "update_every_n_episodes = 1\n",
    "test_every_n_episodes = 100\n",
    "# create folder \n",
    "Path(serialization_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on Device =  cpu\n",
      "Initialized new MCTS1\n",
      "Generating an unbiased Tangram environment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/turan/miniconda3/lib/python3.9/site-packages/torch/cuda/__init__.py:80: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180487213/work/c10/cuda/CUDAFunctions.cpp:112.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connectivity matrix:\n",
      " [[ 0. 16.  6.  6.]\n",
      " [16.  0.  3.  6.]\n",
      " [ 6.  3.  0. 16.]\n",
      " [ 6.  6. 16.  0.]]\n",
      "Uniformity threshhold: 50.50%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if gpu and torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(network_seed)\n",
    "else:\n",
    "    torch.manual_seed(network_seed)\n",
    "    device = \"cpu\"\n",
    "    if gpu:\n",
    "        gpu = False\n",
    "torch.set_num_threads(os.cpu_count() - 1)\n",
    "print(\"Running on Device = \", device)\n",
    "filesave_paths_mcts1 = sorted(glob.glob(f'{serialization_path}/mcts1_e*'))\n",
    "if load_agent and len(filesave_paths_mcts1) > 0:\n",
    "    mcts1 = torch.load(open(filesave_paths_mcts1[-1],'rb'))\n",
    "    n_episodes = int(filesave_paths_mcts1[-1][-30:-24])\n",
    "    running_reward = float(filesave_paths_mcts1[-1][-22:].replace('.pt',''))\n",
    "    print('Loaded MCTS1 from '+ filesave_paths_mcts1[-1])\n",
    "else:\n",
    "    mcts1 = MCTS1(action_dims,\n",
    "                  device).to(device)\n",
    "    n_episodes = 0\n",
    "    running_reward = -1\n",
    "    print('Initialized new MCTS1')\n",
    "env = Tangram(environment_seed, n_grid, n_blocks, n_possible_blocks, chunk_type, n_blocks_H, n_distinct_samples, n_samples)\n",
    "tree = MCTSTree(env)\n",
    "#optimizer_mcts1 = optim.Adam(mcts1.parameters(), lr=5e-4)\n",
    "optimizer_mcts1 = optim.SGD(mcts1.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(tree, n_simuls, mcts1):\n",
    "    probs, action_mask = mcts1(tree, n_simuls, c, gamma)    \n",
    "    probs = torch.squeeze(probs)\n",
    "    action_mask = torch.tensor(action_mask).to(device)\n",
    "    masked_probs = probs*action_mask\n",
    "    if not torch.sum(masked_probs.clone()) > 0:\n",
    "        masked_probs += action_mask\n",
    "    masked_probs /= torch.sum(masked_probs)\n",
    "    m = Categorical(masked_probs)\n",
    "    action = m.sample()\n",
    "    action_id = action.item()\n",
    "    block = action_id//(3*n_blocks)\n",
    "    loc = action_id - block*3*n_blocks\n",
    "    env_action = np.array([block,loc])\n",
    "    #train_probs = masked_probs + 1e-4*(1-action_mask)\n",
    "    #train_probs /= torch.sum(train_probs)\n",
    "    #m_train = Categorical(train_probs)\n",
    "    mcts1.update_saved_log_probs(m.log_prob(action))\n",
    "    mcts1.update_saved_entropies(m.entropy())\n",
    "    return env_action, action_id, m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_episode(mcts1, optimizer, episode_num):\n",
    "    R = 0\n",
    "    mcts1_loss = []\n",
    "    returns = []\n",
    "    for r in mcts1.get_rewards()[::-1]:\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns).to(device)\n",
    "    for log_prob, R in zip(mcts1.get_saved_log_probs(), returns):\n",
    "        mcts1_loss.append(torch.unsqueeze(-log_prob*R,0))\n",
    "    mcts1_loss = torch.cat(mcts1_loss).sum()/update_every_n_episodes\n",
    "    if episode_num % update_every_n_episodes == 0:\n",
    "        optimizer.zero_grad()\n",
    "    mcts1_loss.backward()\n",
    "    #torch.nn.utils.clip_grad_norm_(mcts1.parameters(), 1.0)\n",
    "    if episode_num % update_every_n_episodes == 0:\n",
    "        optimizer.step()\n",
    "        #print('Updated Weights!')\n",
    "    mcts1.update_losses(mcts1_loss.clone().cpu().detach().numpy())\n",
    "    mcts1.delete_rewards()\n",
    "    mcts1.delete_saved_log_probs()\n",
    "    mcts1.delete_saved_entropies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tLast reward: -1.00\tAverage reward: -1.00\tLast loss: -3.22\n",
      "Episode 2\tLast reward: -1.00\tAverage reward: -1.00\tLast loss: -3.21\n",
      "Episode 3\tLast reward: -1.00\tAverage reward: -1.00\tLast loss: -3.66\n",
      "Episode 4\tLast reward: -1.00\tAverage reward: -1.00\tLast loss: -3.95\n",
      "Episode 5\tLast reward: -1.00\tAverage reward: -1.00\tLast loss: -2.67\n",
      "Episode 6\tLast reward: -1.00\tAverage reward: -1.00\tLast loss: -3.58\n",
      "Episode 7\tLast reward: -1.00\tAverage reward: -1.00\tLast loss: -3.33\n",
      "Episode 8\tLast reward: 1.00\tAverage reward: -0.90\tLast loss: 3.29\n",
      "Episode 9\tLast reward: -1.00\tAverage reward: -0.90\tLast loss: -4.00\n",
      "Episode 10\tLast reward: -1.00\tAverage reward: -0.91\tLast loss: -3.74\n",
      "Episode 11\tLast reward: -1.00\tAverage reward: -0.91\tLast loss: -3.40\n",
      "Episode 12\tLast reward: -1.00\tAverage reward: -0.92\tLast loss: -4.35\n",
      "Episode 13\tLast reward: -1.00\tAverage reward: -0.92\tLast loss: -4.82\n",
      "Episode 14\tLast reward: -1.00\tAverage reward: -0.93\tLast loss: -3.26\n",
      "Episode 15\tLast reward: -1.00\tAverage reward: -0.93\tLast loss: -3.56\n",
      "Episode 16\tLast reward: -1.00\tAverage reward: -0.93\tLast loss: -3.81\n",
      "Episode 17\tLast reward: -1.00\tAverage reward: -0.94\tLast loss: -4.59\n",
      "Episode 18\tLast reward: -1.00\tAverage reward: -0.94\tLast loss: -2.65\n",
      "Episode 19\tLast reward: -1.00\tAverage reward: -0.94\tLast loss: -3.45\n",
      "Episode 20\tLast reward: -1.00\tAverage reward: -0.95\tLast loss: -4.00\n",
      "Episode 21\tLast reward: -1.00\tAverage reward: -0.95\tLast loss: -4.55\n",
      "Episode 22\tLast reward: -1.00\tAverage reward: -0.95\tLast loss: -4.67\n",
      "Episode 23\tLast reward: -1.00\tAverage reward: -0.95\tLast loss: -3.24\n",
      "Episode 24\tLast reward: -1.00\tAverage reward: -0.96\tLast loss: -3.89\n",
      "Episode 25\tLast reward: -1.00\tAverage reward: -0.96\tLast loss: -3.20\n",
      "Episode 26\tLast reward: -1.00\tAverage reward: -0.96\tLast loss: -3.44\n",
      "Episode 27\tLast reward: -1.00\tAverage reward: -0.96\tLast loss: -4.01\n",
      "Episode 28\tLast reward: -1.00\tAverage reward: -0.96\tLast loss: -3.52\n",
      "Episode 29\tLast reward: -1.00\tAverage reward: -0.97\tLast loss: -3.23\n",
      "Episode 30\tLast reward: -1.00\tAverage reward: -0.97\tLast loss: -4.61\n",
      "Episode 31\tLast reward: -1.00\tAverage reward: -0.97\tLast loss: -3.54\n",
      "Episode 32\tLast reward: -1.00\tAverage reward: -0.97\tLast loss: -3.69\n",
      "Episode 33\tLast reward: -1.00\tAverage reward: -0.97\tLast loss: -3.84\n",
      "Episode 34\tLast reward: -1.00\tAverage reward: -0.97\tLast loss: -3.47\n",
      "Episode 35\tLast reward: -1.00\tAverage reward: -0.97\tLast loss: -3.49\n",
      "Episode 36\tLast reward: -1.00\tAverage reward: -0.98\tLast loss: -4.07\n",
      "Episode 37\tLast reward: 1.00\tAverage reward: -0.88\tLast loss: 4.01\n",
      "Episode 38\tLast reward: -1.00\tAverage reward: -0.88\tLast loss: -3.90\n",
      "Episode 39\tLast reward: -1.00\tAverage reward: -0.89\tLast loss: -3.13\n",
      "Episode 40\tLast reward: -1.00\tAverage reward: -0.89\tLast loss: -3.98\n",
      "Episode 41\tLast reward: -1.00\tAverage reward: -0.90\tLast loss: -3.21\n",
      "Episode 42\tLast reward: -1.00\tAverage reward: -0.91\tLast loss: -4.28\n",
      "Episode 43\tLast reward: -1.00\tAverage reward: -0.91\tLast loss: -3.14\n",
      "Episode 44\tLast reward: -1.00\tAverage reward: -0.91\tLast loss: -3.19\n",
      "Episode 45\tLast reward: -1.00\tAverage reward: -0.92\tLast loss: -4.13\n",
      "Episode 46\tLast reward: -1.00\tAverage reward: -0.92\tLast loss: -4.58\n",
      "Episode 47\tLast reward: -1.00\tAverage reward: -0.93\tLast loss: -4.04\n",
      "Episode 48\tLast reward: -1.00\tAverage reward: -0.93\tLast loss: -3.84\n",
      "Episode 49\tLast reward: -1.00\tAverage reward: -0.93\tLast loss: -3.54\n",
      "Episode 50\tLast reward: -1.00\tAverage reward: -0.94\tLast loss: -4.16\n",
      "Episode 51\tLast reward: -1.00\tAverage reward: -0.94\tLast loss: -3.38\n",
      "Episode 52\tLast reward: -1.00\tAverage reward: -0.94\tLast loss: -3.04\n",
      "Episode 53\tLast reward: -1.00\tAverage reward: -0.95\tLast loss: -3.58\n",
      "Episode 54\tLast reward: -1.00\tAverage reward: -0.95\tLast loss: -3.99\n",
      "Episode 55\tLast reward: 1.00\tAverage reward: -0.85\tLast loss: 3.74\n",
      "Episode 56\tLast reward: -1.00\tAverage reward: -0.86\tLast loss: -3.32\n",
      "Episode 57\tLast reward: -1.00\tAverage reward: -0.87\tLast loss: -3.45\n",
      "Episode 58\tLast reward: -1.00\tAverage reward: -0.87\tLast loss: -5.13\n",
      "Episode 59\tLast reward: -1.00\tAverage reward: -0.88\tLast loss: -3.75\n",
      "Episode 60\tLast reward: -1.00\tAverage reward: -0.88\tLast loss: -4.25\n",
      "Episode 61\tLast reward: 1.00\tAverage reward: -0.79\tLast loss: 3.93\n",
      "Episode 62\tLast reward: -1.00\tAverage reward: -0.80\tLast loss: -3.84\n",
      "Episode 63\tLast reward: -1.00\tAverage reward: -0.81\tLast loss: -3.00\n",
      "Episode 64\tLast reward: -1.00\tAverage reward: -0.82\tLast loss: -4.26\n",
      "Episode 65\tLast reward: 1.00\tAverage reward: -0.73\tLast loss: 3.91\n",
      "Episode 66\tLast reward: -1.00\tAverage reward: -0.74\tLast loss: -3.25\n",
      "Episode 67\tLast reward: -1.00\tAverage reward: -0.76\tLast loss: -3.34\n",
      "Episode 68\tLast reward: -1.00\tAverage reward: -0.77\tLast loss: -4.09\n",
      "Episode 69\tLast reward: -1.00\tAverage reward: -0.78\tLast loss: -3.90\n",
      "Episode 70\tLast reward: -1.00\tAverage reward: -0.79\tLast loss: -3.54\n",
      "Episode 71\tLast reward: -1.00\tAverage reward: -0.80\tLast loss: -4.37\n",
      "Episode 72\tLast reward: -1.00\tAverage reward: -0.81\tLast loss: -3.72\n",
      "Episode 73\tLast reward: -1.00\tAverage reward: -0.82\tLast loss: -4.26\n",
      "Episode 74\tLast reward: -1.00\tAverage reward: -0.83\tLast loss: -4.78\n",
      "Episode 75\tLast reward: -1.00\tAverage reward: -0.84\tLast loss: -4.57\n",
      "Episode 76\tLast reward: -1.00\tAverage reward: -0.85\tLast loss: -3.76\n",
      "Episode 77\tLast reward: -1.00\tAverage reward: -0.85\tLast loss: -3.91\n",
      "Episode 78\tLast reward: -1.00\tAverage reward: -0.86\tLast loss: -4.09\n",
      "Episode 79\tLast reward: -1.00\tAverage reward: -0.87\tLast loss: -3.35\n",
      "Episode 80\tLast reward: -1.00\tAverage reward: -0.87\tLast loss: -4.31\n",
      "Episode 81\tLast reward: -1.00\tAverage reward: -0.88\tLast loss: -4.24\n",
      "Episode 82\tLast reward: -1.00\tAverage reward: -0.89\tLast loss: -4.25\n",
      "Episode 83\tLast reward: 1.00\tAverage reward: -0.79\tLast loss: 4.38\n",
      "Episode 84\tLast reward: -1.00\tAverage reward: -0.80\tLast loss: -4.08\n",
      "Episode 85\tLast reward: -1.00\tAverage reward: -0.81\tLast loss: -5.02\n",
      "Episode 86\tLast reward: -1.00\tAverage reward: -0.82\tLast loss: -3.43\n",
      "Episode 87\tLast reward: -1.00\tAverage reward: -0.83\tLast loss: -5.97\n",
      "Episode 88\tLast reward: -1.00\tAverage reward: -0.84\tLast loss: -3.55\n",
      "Episode 89\tLast reward: -1.00\tAverage reward: -0.85\tLast loss: -4.87\n",
      "Episode 90\tLast reward: -1.00\tAverage reward: -0.86\tLast loss: -5.15\n",
      "Episode 91\tLast reward: -1.00\tAverage reward: -0.86\tLast loss: -3.34\n",
      "Episode 92\tLast reward: 1.00\tAverage reward: -0.77\tLast loss: 4.19\n",
      "Episode 93\tLast reward: -1.00\tAverage reward: -0.78\tLast loss: -2.58\n",
      "Episode 94\tLast reward: 1.00\tAverage reward: -0.69\tLast loss: 3.08\n",
      "Episode 95\tLast reward: -1.00\tAverage reward: -0.71\tLast loss: -3.48\n",
      "Episode 96\tLast reward: -1.00\tAverage reward: -0.72\tLast loss: -3.55\n",
      "Episode 97\tLast reward: -1.00\tAverage reward: -0.74\tLast loss: -3.29\n",
      "Episode 98\tLast reward: -1.00\tAverage reward: -0.75\tLast loss: -3.70\n",
      "Episode 99\tLast reward: -1.00\tAverage reward: -0.76\tLast loss: -3.21\n",
      "Episode 100\tLast reward: -1.00\tAverage reward: -0.77\tLast loss: -3.31\n",
      "Testing...\n",
      "Success ratio: 11.0%\n",
      "Episode 101\tLast reward: -1.00\tAverage reward: -0.78\tLast loss: -4.10\n",
      "Episode 102\tLast reward: -1.00\tAverage reward: -0.80\tLast loss: -4.55\n",
      "Episode 103\tLast reward: -1.00\tAverage reward: -0.81\tLast loss: -3.66\n",
      "Episode 104\tLast reward: -1.00\tAverage reward: -0.82\tLast loss: -4.41\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_167492/982016343.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mep_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_blocks\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0menv_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_simuls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmcts1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#probs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0menv_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchild_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_children\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_167492/899809129.py\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(tree, n_simuls, mcts1)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_simuls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmcts1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmcts1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_simuls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0maction_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmasked_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0maction_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/master-thesis/learning-granular-planning/mcts1.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tree, n_simuls, c, gamma)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                     \u001b[0;31m#if N[action] > 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                     \u001b[0mmasked_Q\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_Q\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_Q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0mcurrent_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reward_threshold = 0.99\n",
    "success_threshold = 99\n",
    "success_ratio = 0\n",
    "for i_episode in count(n_episodes+1):\n",
    "    env.reset('train')\n",
    "    if render:\n",
    "        env.render()\n",
    "    tree = MCTSTree(env)\n",
    "    ep_reward = 0\n",
    "    for t in range(1, n_blocks + 1):\n",
    "        env_action, action, log_prob = select_action(tree, n_simuls, mcts1) #probs\n",
    "        env_state, env_reward, done = env.step(env_action)\n",
    "        for (child_id, child) in tree.get_root().get_children():\n",
    "            if child_id == action:\n",
    "                child.set_reward(env_reward)\n",
    "                child.set_done(done)\n",
    "                child.set_action_mask(tree.get_env().get_mask())\n",
    "                tree.set_root(child)\n",
    "            else:\n",
    "                child.delete()\n",
    "        if render:\n",
    "            env.render()\n",
    "        mcts1.update_rewards(env_reward)\n",
    "        ep_reward += env_reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "    mcts1.update_running_rewards(running_reward)\n",
    "    finish_episode(mcts1, optimizer_mcts1, i_episode)\n",
    "\n",
    "    if i_episode % log_interval == 0:\n",
    "        print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}\\tLast loss: {:.2f}'.format(\n",
    "            i_episode, ep_reward, running_reward, mcts1.get_last_episode_loss()))\n",
    "    if serialize_every_n_episodes > 0 and i_episode % serialize_every_n_episodes == 0:\n",
    "        torch.save(mcts1, f\"{serialization_path}/mcts1_e{str(i_episode).zfill(6)}_p{running_reward}.pt\")\n",
    "        print(\"Saved the model!\")\n",
    "        del mcts1, optimizer_mcts1\n",
    "        filesave_paths_mcts1 = sorted(glob.glob(f'{serialization_path}/mcts1_e*'))\n",
    "        mcts1 = torch.load(open(filesave_paths_mcts1[-1],'rb'))\n",
    "        n_episodes = int(filesave_paths_mcts1[-1][-30:-24])\n",
    "        running_reward = float(filesave_paths_mcts1[-1][-22:].replace('.pt',''))\n",
    "        print('Loaded MCTS1 from '+ filesave_paths_mcts1[-1])\n",
    "        #optimizer_mcts1 = optim.Adam(mcts1.parameters(), lr=5e-4)\n",
    "        optimizer_mcts1 = optim.SGD(mcts1.parameters(), lr=5e-4)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    if i_episode % test_every_n_episodes == 0:\n",
    "        print('Testing...')\n",
    "        mcts1.eval()\n",
    "        with torch.no_grad():\n",
    "            success_ratio = 0\n",
    "            for eval_num in range(1,n_evals+1):\n",
    "                env.reset('test')\n",
    "                tree = MCTSTree(env)\n",
    "                done = False\n",
    "                while not done:\n",
    "                    if render:\n",
    "                        env.render()\n",
    "                    probs, action_mask = mcts1(tree, n_simuls, c, gamma)\n",
    "                    probs = torch.squeeze(probs)\n",
    "                    action_mask = torch.tensor(action_mask).to(device)\n",
    "                    masked_probs = probs*action_mask \n",
    "                    if not torch.sum(masked_probs.clone()) > 0:\n",
    "                        masked_probs += action_mask\n",
    "                    masked_probs /= torch.sum(masked_probs)\n",
    "                    m = Categorical(masked_probs)\n",
    "                    #action = m.sample().item()\n",
    "                    action = torch.argmax(masked_probs)\n",
    "                    action_id = action.item()\n",
    "                    block = action_id//(3*n_blocks)\n",
    "                    loc = action_id - block*3*n_blocks\n",
    "                    env_action = np.array([block,loc])\n",
    "                    env_state, env_reward, done = env.step(env_action)\n",
    "\n",
    "                    for (child_id, child) in tree.get_root().get_children():\n",
    "                        if child_id == action_id:\n",
    "                            child.set_reward(env_reward)\n",
    "                            child.set_done(done)\n",
    "                            child.set_action_mask(tree.get_env().get_mask())\n",
    "                            tree.set_root(child)\n",
    "                        else:\n",
    "                            child.delete()\n",
    "                if env_reward == 1:\n",
    "                    success_ratio += 1\n",
    "                if render:\n",
    "                    env.render()\n",
    "            success_ratio *= 100/n_evals\n",
    "            print(\"Success ratio: {}%\".format(success_ratio))\n",
    "            mcts1.update_success_ratios(success_ratio)\n",
    "        mcts1.train()\n",
    "    if running_reward > reward_threshold or success_ratio > success_threshold:\n",
    "        print(\"Solved! Running reward is now {} and \"\n",
    "                \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "        torch.save(mcts1, f\"{serialization_path}/mcts1_e{str(i_episode).zfill(6)}_p{running_reward}.pt\")\n",
    "        print(\"Saved the model!\")\n",
    "        break\n",
    "    tree.delete()\n",
    "    del tree\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mcts1, f\"{serialization_path}/mcts1_e{str(i_episode).zfill(6)}_p{running_reward}.pt\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cd80e2a333356e0627c76a098ec269248fb9c7e6d0398da160cf165fdbf4bf09"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
