{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from torch.distributions import Categorical\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import gc\n",
    "\n",
    "from mctsnet import MCTSnet\n",
    "from mnetwork import MNetwork\n",
    "from internal_m import InternalM\n",
    "from tangram import Tangram\n",
    "from mctsnettree import MCTSnetTree\n",
    "from itertools import count\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serialization_path:  ./models/mctsnet/hierarchical_blocks_0\n"
     ]
    }
   ],
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "gamma=0.9\n",
    "alpha=0.05\n",
    "tau=0.1\n",
    "M_penalty=-10\n",
    "seed=543\n",
    "render=False\n",
    "log_interval=1\n",
    "gpu=False\n",
    "load_agent=True\n",
    "meta_control=False\n",
    "internal_control=False\n",
    "load_M=True\n",
    "\n",
    "n_grid = 20\n",
    "n_blocks = 4\n",
    "n_possible_blocks = 6\n",
    "chunk_type = 7\n",
    "n_blocks_H = 0\n",
    "n_samples = 40\n",
    "\n",
    "embedding_size = 128\n",
    "readout_hidden_size = 128\n",
    "backup_hidden_size = 128\n",
    "action_dims = [3,n_blocks,n_possible_blocks]\n",
    "state_dims = [2,n_grid,n_grid]\n",
    "embedding_n_residual_blocks = 3\n",
    "embedding_channel_sizes = [64,64,64,32]\n",
    "embedding_kernels = [3,3,3,1]\n",
    "embedding_strides = [1,1,1,1]\n",
    "policy_n_residual_blocks = 2\n",
    "policy_channel_sizes = [32,32,32,16]\n",
    "policy_kernels = [3,3,3,1]\n",
    "policy_strides = [1,1,1,1]\n",
    "policy_hidden_size = 128\n",
    "M_n_residual_blocks = 2\n",
    "M_channel_sizes = [2,2,2,1]\n",
    "M_kernels = [3,3,3,1]\n",
    "M_strides = [1,1,1,1]\n",
    "M_hidden_size = 8\n",
    "internal_m_hidden_size = 8\n",
    "max_M = 20\n",
    "M = None\n",
    "\n",
    "n_simuls = 10\n",
    "n_evals = 100\n",
    "\n",
    "serialization_path = './models/mctsnet/hierarchical_blocks_{}'.format(n_blocks_H)\n",
    "print('serialization_path: ',serialization_path)\n",
    "serialize_every_n_episodes = 10000\n",
    "update_every_n_episodes = 1\n",
    "test_every_n_episodes = 100\n",
    "# create folder \n",
    "Path(serialization_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on Device =  cpu\n",
      "Loaded MCTSnet from ./models/mctsnet/hierarchical_blocks_0/mctsnet_e014846_p-0.17715903665656524.pt\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if gpu and torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "else:\n",
    "    torch.manual_seed(seed)\n",
    "    device = \"cpu\"\n",
    "    if gpu:\n",
    "        gpu = False\n",
    "torch.set_num_threads(os.cpu_count() - 1)\n",
    "print(\"Running on Device = \", device)\n",
    "filesave_paths_mctsnet = sorted(glob.glob(f'{serialization_path}/mctsnet_e*'))\n",
    "filesave_paths_M = sorted(glob.glob(f'{serialization_path}/M_e*'))\n",
    "if load_agent and len(filesave_paths_mctsnet) > 0:\n",
    "    mctsnet = torch.load(open(filesave_paths_mctsnet[-1],'rb'), map_location=device)\n",
    "    n_episodes = int(filesave_paths_mctsnet[-1][48:54])\n",
    "    running_reward = float(filesave_paths_mctsnet[-1][56:].replace('.pt',''))\n",
    "    print('Loaded MCTSnet from '+ filesave_paths_mctsnet[-1])\n",
    "else:\n",
    "    mctsnet = MCTSnet(embedding_size,\n",
    "                      readout_hidden_size,\n",
    "                      backup_hidden_size,\n",
    "                      action_dims,\n",
    "                      state_dims,\n",
    "                      embedding_n_residual_blocks,\n",
    "                      embedding_channel_sizes,\n",
    "                      embedding_kernels,\n",
    "                      embedding_strides,\n",
    "                      policy_n_residual_blocks,\n",
    "                      policy_channel_sizes,\n",
    "                      policy_kernels,\n",
    "                      policy_strides,\n",
    "                      policy_hidden_size,\n",
    "                      device).to(device)\n",
    "    n_episodes = 0\n",
    "    running_reward = -1\n",
    "    print('Initialized new MCTSnet')\n",
    "if meta_control:\n",
    "    if load_M and len(filesave_paths_M) > 0:\n",
    "        M = torch.load(open(filesave_paths_M[-1],'rb'), map_location=device)\n",
    "        print('Loaded MNetwork from '+ filesave_paths_M[-1])\n",
    "    elif internal_control:\n",
    "        M = InternalM(embedding_size, internal_m_hidden_size).to(device)\n",
    "    else:\n",
    "        M = MNetwork(state_dims,\n",
    "                     M_n_residual_blocks,\n",
    "                     M_channel_sizes,\n",
    "                     M_kernels,\n",
    "                     M_strides,\n",
    "                     M_hidden_size,\n",
    "                     max_M).to(device)\n",
    "        print('Initialized new MNetwork')\n",
    "seed += n_episodes\n",
    "env = Tangram(seed, n_grid, n_blocks, n_possible_blocks, chunk_type, n_blocks_H, n_samples)\n",
    "tree = MCTSnetTree(env, embedding_size, device)\n",
    "#optimizer_mctsnet = optim.Adam(mctsnet.parameters(), lr=5e-4)\n",
    "optimizer_mctsnet = optim.SGD(mctsnet.parameters(), lr=5e-4)\n",
    "if meta_control:\n",
    "    #optimizer_M = optim.Adam(M.parameters(), lr=5e-4)\n",
    "    optimizer_M = optim.SGD(M.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(tree, n_simuls, mctsnet):\n",
    "    probs, action_mask = mctsnet(tree, M, n_simuls, gamma, internal_control)    \n",
    "    probs = torch.squeeze(probs)\n",
    "    action_mask = torch.tensor(action_mask).to(device)\n",
    "    masked_probs = probs*action_mask\n",
    "    if not torch.sum(masked_probs.clone()) > 0:\n",
    "        masked_probs += action_mask\n",
    "    masked_probs /= torch.sum(masked_probs)\n",
    "    m = Categorical(masked_probs)\n",
    "    action = m.sample()\n",
    "    action_id = action.item()\n",
    "    block = action_id//(3*n_blocks)\n",
    "    loc = action_id - block*3*n_blocks\n",
    "    env_action = np.array([block,loc])\n",
    "    #train_probs = masked_probs + 1e-4*(1-action_mask)\n",
    "    #train_probs /= torch.sum(train_probs)\n",
    "    #m_train = Categorical(train_probs)\n",
    "    mctsnet.update_saved_log_probs(m.log_prob(action))\n",
    "    mctsnet.update_saved_entropies(m.entropy())\n",
    "    return env_action, action_id, m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_episode(mctsnet, optimizer, episode_num, *args):\n",
    "    R = 0\n",
    "    mctsnet_loss = []\n",
    "    returns = []\n",
    "    for r in mctsnet.get_rewards()[::-1]:\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns).to(device)\n",
    "    for log_prob, R, entropy in zip(mctsnet.get_saved_log_probs(), returns, mctsnet.get_saved_entropies()):\n",
    "        mctsnet_loss.append(torch.unsqueeze(-log_prob*R - tau*entropy,0))\n",
    "    mctsnet_loss = torch.cat(mctsnet_loss).sum()/update_every_n_episodes\n",
    "    if episode_num % update_every_n_episodes == 0:\n",
    "        optimizer.zero_grad()\n",
    "    mctsnet_loss.backward()\n",
    "    #torch.nn.utils.clip_grad_norm_(mctsnet.parameters(), 1.0)\n",
    "    if episode_num % update_every_n_episodes == 0:\n",
    "        optimizer.step()\n",
    "        #print('Updated Weights!')\n",
    "    mctsnet.update_losses(mctsnet_loss.clone().cpu().detach().numpy())\n",
    "    mctsnet.delete_rewards()\n",
    "    mctsnet.delete_saved_log_probs()\n",
    "    mctsnet.delete_saved_entropies()\n",
    "    if meta_control:\n",
    "        M = args[0]\n",
    "        optimizer_M = args[1]\n",
    "        R = 0\n",
    "        M_loss = []\n",
    "        returns = []\n",
    "        for r in M.get_rewards()[::-1]:\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns).to(device)\n",
    "        for log_prob, R in zip(M.get_saved_log_probs(), returns):\n",
    "            M_loss.append(torch.unsqueeze(-log_prob*R,0))\n",
    "        optimizer_M.zero_grad()\n",
    "        M_loss = torch.cat(M_loss).sum()\n",
    "        M_loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(M.parameters(), 1.0)\n",
    "        optimizer_M.step()\n",
    "        M.update_losses(M_loss.clone().cpu().detach().numpy())\n",
    "        M.delete_rewards()\n",
    "        M.delete_saved_log_probs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 14847\tLast reward: 1.00\tAverage reward: -0.12\tLast loss: 5.19\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "tensor(36.2544)\n",
      "Episode 14848\tLast reward: 1.00\tAverage reward: -0.06\tLast loss: 1.04\n",
      "tensor(110.1244)\n",
      "tensor(110.1244)\n",
      "tensor(110.1244)\n",
      "tensor(110.1244)\n",
      "tensor(110.1244)\n",
      "tensor(110.1244)\n",
      "tensor(110.1244)\n",
      "tensor(110.1244)\n",
      "tensor(110.1244)\n",
      "tensor(110.1244)\n",
      "tensor(110.1244)\n",
      "tensor(110.1244)\n",
      "tensor(110.1244)\n",
      "tensor(110.1244)\n",
      "tensor(110.1244)\n",
      "tensor(110.1244)\n",
      "tensor(110.1244)\n",
      "tensor(110.1244)\n",
      "tensor(110.1244)\n",
      "tensor(110.1244)\n",
      "tensor(110.1244)\n",
      "tensor(110.1244)\n",
      "tensor(110.1244)\n",
      "tensor(110.1244)\n",
      "tensor(110.1244)\n",
      "tensor(110.1244)\n",
      "tensor(110.1244)\n",
      "tensor(110.1244)\n",
      "tensor(110.1244)\n",
      "tensor(110.1244)\n",
      "tensor(110.1244)\n",
      "tensor(110.1244)\n",
      "tensor(110.1244)\n",
      "tensor(110.1244)\n",
      "tensor(110.1244)\n",
      "Episode 14849\tLast reward: -1.00\tAverage reward: -0.11\tLast loss: -3.18\n",
      "tensor(103.6997)\n",
      "tensor(103.6997)\n",
      "tensor(103.6997)\n",
      "tensor(103.6997)\n",
      "tensor(103.6997)\n",
      "tensor(103.6997)\n",
      "tensor(103.6997)\n",
      "tensor(103.6997)\n",
      "tensor(103.6997)\n",
      "tensor(103.6997)\n",
      "tensor(103.6997)\n",
      "tensor(103.6997)\n",
      "tensor(103.6997)\n",
      "tensor(103.6997)\n",
      "tensor(103.6997)\n",
      "tensor(103.6997)\n",
      "tensor(103.6997)\n",
      "tensor(103.6997)\n",
      "tensor(103.6997)\n",
      "tensor(103.6997)\n",
      "tensor(103.6997)\n",
      "tensor(103.6997)\n",
      "tensor(103.6997)\n",
      "tensor(103.6997)\n",
      "tensor(103.6997)\n",
      "tensor(103.6997)\n",
      "tensor(103.6997)\n",
      "tensor(103.6997)\n",
      "tensor(103.6997)\n",
      "tensor(103.6997)\n",
      "tensor(103.6997)\n",
      "tensor(103.6997)\n",
      "tensor(103.6997)\n",
      "tensor(103.6997)\n",
      "tensor(103.6997)\n",
      "tensor(103.6997)\n",
      "Episode 14850\tLast reward: -1.00\tAverage reward: -0.15\tLast loss: -0.94\n",
      "tensor(28.1287)\n",
      "tensor(28.1287)\n",
      "tensor(28.1287)\n",
      "tensor(28.1287)\n",
      "tensor(28.1287)\n",
      "tensor(28.1287)\n",
      "tensor(28.1287)\n",
      "tensor(28.1287)\n",
      "tensor(28.1287)\n",
      "tensor(28.1287)\n",
      "tensor(28.1287)\n",
      "tensor(28.1287)\n",
      "tensor(28.1287)\n",
      "tensor(28.1287)\n",
      "tensor(28.1287)\n",
      "tensor(28.1287)\n",
      "tensor(28.1287)\n",
      "tensor(28.1287)\n",
      "tensor(28.1287)\n",
      "tensor(28.1287)\n",
      "tensor(28.1287)\n",
      "tensor(28.1287)\n",
      "tensor(28.1287)\n",
      "tensor(28.1287)\n",
      "tensor(28.1287)\n",
      "tensor(28.1287)\n",
      "tensor(28.1287)\n",
      "tensor(28.1287)\n",
      "tensor(28.1287)\n",
      "tensor(28.1287)\n",
      "tensor(28.1287)\n",
      "tensor(28.1287)\n",
      "tensor(28.1287)\n",
      "tensor(28.1287)\n",
      "Episode 14851\tLast reward: -1.00\tAverage reward: -0.20\tLast loss: -2.23\n",
      "tensor(63.8534)\n",
      "tensor(63.8534)\n",
      "tensor(63.8534)\n",
      "tensor(63.8534)\n",
      "tensor(63.8534)\n",
      "tensor(63.8534)\n",
      "tensor(63.8534)\n",
      "tensor(63.8534)\n",
      "tensor(63.8534)\n",
      "tensor(63.8534)\n",
      "tensor(63.8534)\n",
      "tensor(63.8534)\n",
      "tensor(63.8534)\n",
      "tensor(63.8534)\n",
      "tensor(63.8534)\n",
      "tensor(63.8534)\n",
      "tensor(63.8534)\n",
      "tensor(63.8534)\n",
      "tensor(63.8534)\n",
      "tensor(63.8534)\n",
      "tensor(63.8534)\n",
      "tensor(63.8534)\n",
      "tensor(63.8534)\n",
      "tensor(63.8534)\n",
      "tensor(63.8534)\n",
      "tensor(63.8534)\n",
      "tensor(63.8534)\n",
      "tensor(63.8534)\n",
      "tensor(63.8534)\n",
      "tensor(63.8534)\n",
      "tensor(63.8534)\n",
      "tensor(63.8534)\n",
      "tensor(63.8534)\n",
      "tensor(63.8534)\n",
      "tensor(63.8534)\n",
      "tensor(63.8534)\n",
      "Episode 14852\tLast reward: -1.00\tAverage reward: -0.24\tLast loss: -1.35\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "tensor(33.6514)\n",
      "Episode 14853\tLast reward: 1.00\tAverage reward: -0.17\tLast loss: 2.22\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n",
      "tensor(72.0084)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27060/3969432462.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mn_simuls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mM_action\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_saved_log_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM_m\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0menv_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_simuls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmctsnet\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#probs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmeta_control\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minternal_control\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mn_simuls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_saved_n_simuls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_27060/1993611215.py\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(tree, n_simuls, mctsnet)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_simuls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmctsnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmctsnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_simuls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minternal_control\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0maction_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmasked_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0maction_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/master-thesis/learning-granular-planning/mctsnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tree, internal_M, n_simuls, gamma, internal_control)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \u001b[0;31m#R = reward + gamma*R\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                 \u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_h\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__backup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_prime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m                 \u001b[0;31m#print(parent.get_h()-h)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0mcurrent_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/master-thesis/learning-granular-planning/networks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, h, h_prime, reward, action)\u001b[0m\n\u001b[1;32m     56\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__g\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \"\"\"\n\u001b[1;32m     60\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mBackupNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/traceback.py\u001b[0m in \u001b[0;36mformat_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;34m\"\"\"Shorthand for 'format_list(extract_stack(f, limit))'.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mformat_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reward_threshold = 0.99\n",
    "success_threshold = 99\n",
    "success_ratio = 0\n",
    "for i_episode in count(n_episodes+1):\n",
    "    env.reset('train')\n",
    "    if render:\n",
    "        env.render()\n",
    "    tree = MCTSnetTree(env, embedding_size, device)\n",
    "    ep_reward = 0\n",
    "    for t in range(1, n_blocks + 1):\n",
    "        if meta_control and not internal_control:\n",
    "            M_probs = M(tree.get_root().get_state())\n",
    "            M_probs = torch.squeeze(M_probs)\n",
    "            M_m = Categorical(M_probs)\n",
    "            M_action = M_m.sample()\n",
    "            n_simuls = M_action.item()+1\n",
    "            M.update_saved_log_probs(M_m.log_prob(M_action))\n",
    "        env_action, action, log_prob = select_action(tree, n_simuls, mctsnet) #probs\n",
    "        if meta_control and internal_control:\n",
    "            n_simuls = M.get_saved_n_simuls()[-1]\n",
    "        env_state, env_reward, done = env.step(env_action)\n",
    "        state = torch.unsqueeze(torch.tensor(env_state[:2]), 0).to(device)\n",
    "        reward = torch.unsqueeze(torch.tensor([env_reward]), 0).to(device)\n",
    "        for (child_id, child) in tree.get_root().get_children():\n",
    "            if child_id == action:\n",
    "                child.set_state(state)\n",
    "                child.set_reward(reward)\n",
    "                child.set_done(done)\n",
    "                child.set_action(torch.reshape(-log_prob, (1,1))) #torch.reshape(probs[action], (1,1))\n",
    "                child.set_action_mask(tree.get_env().get_mask())\n",
    "                tree.set_root(child)\n",
    "                break\n",
    "        if render:\n",
    "            env.render()\n",
    "        mctsnet.update_rewards(reward)\n",
    "        if meta_control:\n",
    "            if env_reward > 0:\n",
    "                M_reward = 1-alpha*n_simuls\n",
    "            elif env_reward < 0:\n",
    "                M_reward = M_penalty\n",
    "            else:\n",
    "                M_reward = 0\n",
    "            M_reward = torch.unsqueeze(torch.tensor([M_reward]), 0).to(device)\n",
    "            M.update_rewards(M_reward)\n",
    "        ep_reward += env_reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "    mctsnet.update_running_rewards(running_reward)\n",
    "    if meta_control:\n",
    "        finish_episode(mctsnet, optimizer_mctsnet, i_episode, M, optimizer_M)\n",
    "    else:\n",
    "        finish_episode(mctsnet, optimizer_mctsnet, i_episode)\n",
    "\n",
    "    if i_episode % log_interval == 0:\n",
    "        if meta_control:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}\\tLast loss: {:.2f}   \\tLast M: {}\\tLast M loss: {:.2f}'.format(\n",
    "                    i_episode, ep_reward, running_reward, mctsnet.get_last_episode_loss(), n_simuls, M.get_last_episode_loss()))\n",
    "        else:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}\\tLast loss: {:.2f}'.format(\n",
    "                i_episode, ep_reward, running_reward, mctsnet.get_last_episode_loss()))\n",
    "    if serialize_every_n_episodes > 0 and i_episode % serialize_every_n_episodes == 0:\n",
    "        torch.save(mctsnet, f\"{serialization_path}/mctsnet_e{str(i_episode).zfill(6)}_p{running_reward}.pt\")\n",
    "        print(\"Saved the model!\")\n",
    "        del mctsnet, optimizer_mctsnet\n",
    "        filesave_paths_mctsnet = sorted(glob.glob(f'{serialization_path}/mctsnet_e*'))\n",
    "        mctsnet = torch.load(open(filesave_paths_mctsnet[-1],'rb'), map_location=device)\n",
    "        n_episodes = int(filesave_paths_mctsnet[-1][48:54])\n",
    "        running_reward = float(filesave_paths_mctsnet[-1][56:].replace('.pt',''))\n",
    "        print('Loaded MCTSnet from '+ filesave_paths_mctsnet[-1])\n",
    "        #optimizer_mctsnet = optim.Adam(mctsnet.parameters(), lr=5e-4)\n",
    "        optimizer_mctsnet = optim.SGD(mctsnet.parameters(), lr=5e-4)\n",
    "        if meta_control:\n",
    "            torch.save(M, f\"{serialization_path}/M_e{str(i_episode).zfill(6)}_p{running_reward}.pt\")\n",
    "            del M, optimizer_M\n",
    "            filesave_paths_M = sorted(glob.glob(f'{serialization_path}/M_e*'))\n",
    "            M = torch.load(open(filesave_paths_M[-1],'rb'), map_location=device)\n",
    "            print('Loaded M from '+ filesave_paths_M[-1])\n",
    "            #optimizer_M = optim.Adam(M.parameters(), lr=5e-4)\n",
    "            optimizer_M = optim.SGD(M.parameters(), lr=5e-4)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    if i_episode % test_every_n_episodes == 0:\n",
    "        print('Testing...')\n",
    "        mctsnet.eval()\n",
    "        if meta_control:\n",
    "            M.eval()\n",
    "        with torch.no_grad():\n",
    "            success_ratio = 0\n",
    "            for eval_num in range(1,n_evals+1):\n",
    "                env.reset('test')\n",
    "                tree = MCTSnetTree(env, embedding_size, device)\n",
    "                done = False\n",
    "                while not done:\n",
    "                    if render:\n",
    "                        env.render()\n",
    "                    if meta_control and not internal_control:\n",
    "                        M_probs = M(tree.get_root().get_state())\n",
    "                        M_probs = torch.squeeze(M_probs)\n",
    "                        #M_m = Categorical(M_probs)\n",
    "                        #M_action = M_m.sample()\n",
    "                        #n_simuls = M_action.item()+1\n",
    "                        n_simuls = torch.argmax(M_probs).item()+1\n",
    "                    probs, action_mask = mctsnet(tree, M, n_simuls, gamma, internal_control)\n",
    "                    probs = torch.squeeze(probs)\n",
    "                    action_mask = torch.tensor(action_mask).to(device)\n",
    "                    masked_probs = probs*action_mask \n",
    "                    if not torch.sum(masked_probs.clone()) > 0:\n",
    "                        masked_probs += action_mask\n",
    "                    masked_probs /= torch.sum(masked_probs)\n",
    "                    m = Categorical(masked_probs)\n",
    "                    #action = m.sample().item()\n",
    "                    action = torch.argmax(masked_probs)\n",
    "                    action_id = action.item()\n",
    "                    block = action_id//(3*n_blocks)\n",
    "                    loc = action_id - block*3*n_blocks\n",
    "                    env_action = np.array([block,loc])\n",
    "                    if meta_control and internal_control:\n",
    "                        n_simuls = M.get_saved_n_simuls()[-1]\n",
    "                    env_state, env_reward, done = env.step(env_action)\n",
    "\n",
    "                    state = torch.unsqueeze(torch.tensor(env_state[:2]), 0).to(device)\n",
    "                    reward = torch.unsqueeze(torch.tensor([env_reward]), 0).to(device)\n",
    "                    for (child_id, child) in tree.get_root().get_children():\n",
    "                        if child_id == action_id:\n",
    "                            child.set_state(state)\n",
    "                            child.set_reward(reward)\n",
    "                            child.set_done(done)\n",
    "                            child.set_action(torch.reshape(-m.log_prob(action), (1,1))) #torch.reshape(probs[action_id], (1,1))\n",
    "                            child.set_action_mask(tree.get_env().get_mask())\n",
    "                            tree.set_root(child)\n",
    "                            break\n",
    "                if env_reward == 1:\n",
    "                    success_ratio += 1\n",
    "                if render:\n",
    "                    env.render()\n",
    "            success_ratio *= 100/n_evals\n",
    "            print(\"Success ratio: {}%\".format(success_ratio))\n",
    "            mctsnet.update_success_ratios(success_ratio)\n",
    "        mctsnet.train()\n",
    "        if meta_control:\n",
    "            M.train()\n",
    "    if running_reward > reward_threshold or success_ratio > success_threshold:\n",
    "        print(\"Solved! Running reward is now {} and \"\n",
    "                \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "        torch.save(mctsnet, f\"{serialization_path}/mctsnet_e{str(i_episode).zfill(6)}_p{running_reward}.pt\")\n",
    "        if meta_control:\n",
    "            torch.save(M, f\"{serialization_path}/M_e{str(i_episode).zfill(6)}_p{running_reward}.pt\")\n",
    "        print(\"Saved the model!\")\n",
    "        break\n",
    "    tree.delete()\n",
    "    del tree\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mctsnet, f\"{serialization_path}/mctsnet_e{str(i_episode).zfill(6)}_p{running_reward}.pt\")\n",
    "if meta_control:\n",
    "    torch.save(M, f\"{serialization_path}/M_e{str(i_episode).zfill(6)}_p{running_reward}.pt\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cd80e2a333356e0627c76a098ec269248fb9c7e6d0398da160cf165fdbf4bf09"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
